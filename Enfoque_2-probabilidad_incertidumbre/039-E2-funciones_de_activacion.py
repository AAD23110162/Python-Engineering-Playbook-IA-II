"""
039-E2-funciones_de_activacion.py
--------------------------------
Este script presenta Funciones de Activación para redes neuronales:
- Sigmoide, tanh, ReLU, Leaky ReLU, ELU y Softmax a nivel conceptual.
- Propiedades: saturación, derivadas y problemas de gradiente.
- Elección de activaciones según la tarea (clasificación/regresión).
- Variables y funciones en español.

El programa puede ejecutarse en dos modos:
1. DEMO: curvas y derivadas de activaciones con parámetros predefinidos.
2. INTERACTIVO: experimentar con combinaciones en capas simples.

Autor: Alejandro Aguirre Díaz
"""
